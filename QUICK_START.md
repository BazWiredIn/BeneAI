# BeneAI Quick Start Guide

## How to Run the Working System

### Option 1: Using `start_demo.sh` (Recommended)

This is the **only working method** right now.

```bash
# From project root
./start_demo.sh
```

This script will:
1. Start backend on `http://localhost:8000`
2. Start frontend on `http://localhost:8080`
3. Open your browser to `http://localhost:8080`

### Option 2: Manual Start

```bash
# Terminal 1: Start backend
cd backend
python main.py

# Terminal 2: Start frontend
cd frontend
python3 -m http.server 8080

# Browser: Open http://localhost:8080
```

---

## When You Press "Start Session" - What Happens?

**YES, you WILL be gathering data!** Here's the exact flow:

### 1. Initialization (1-2 seconds)
- âœ… **Webcam starts** â†’ Requests camera permission
- âœ… **Video processor initializes** â†’ Loads MediaPipe Face Mesh
- âœ… **Audio analyzer starts** â†’ Initializes Web Speech API
- âœ… **WebSocket connects** â†’ Connects to `ws://localhost:8000/ws`

### 2. Data Collection Begins (Continuous)
- ðŸ“¹ **Video frames captured** at 30 FPS â†’ Throttled to 3 FPS
- ðŸ“¤ **Frames sent to backend** via WebSocket (Base64 JPEG, ~60KB each)
- ðŸ¤– **Hume AI analyzes emotions** â†’ Returns 48 emotion scores
- ðŸŽ¯ **Investor state mapped** â†’ skeptical/evaluative/receptive/positive/neutral
- ðŸ“Š **1-second intervals aggregated** â†’ EMA smoothing applied
- ðŸ—£ï¸ **Speech transcribed** â†’ Web Speech API converts audio to text
- ðŸ’¾ **Buffered for 5 seconds** â†’ Rolling window maintained
- ðŸ§  **GPT-4 generates coaching** â†’ Every 4.5 seconds
- ðŸ’¬ **Advice displayed** â†’ Updates UI in real-time

### 3. What You'll See

**In the UI:**
- **Emotion label** â†’ Shows current investor state (e.g., "Receptive")
- **Confidence bar** â†’ Shows Hume AI detection confidence
- **Speech metrics** â†’ Words/min, filler words, pause frequency
- **AI Coaching** â†’ Real-time advice from GPT-4

**In the console (if you open DevTools):**
```
Connected to BeneAI backend
Received: connection
Welcome: Welcome to BeneAI coaching service
Video frame sent
Emotion detected: receptive (Joy: 0.85)
Interval complete: receptive, 3 frames analyzed
LLM coaching: "Great momentum! Investor is engaged..."
```

---

## What Data Is Being Collected?

### Sent to Backend:
1. **Video frames** (Base64 JPEG, 3 FPS)
2. **Transcribed speech** (text only, no audio files)
3. **Speech metrics** (WPM, filler words, pause frequency)

### Processed by Backend:
1. **Hume AI emotion analysis** â†’ 48 emotion scores
2. **Investor state** â†’ One of 6 states
3. **1-second intervals** â†’ Aggregated with EMA smoothing
4. **5-second buffer** â†’ Rolling window for LLM context
5. **GPT-4 coaching advice** â†’ Generated every 4.5 seconds

### Displayed in UI:
1. **Current investor state** (e.g., "Receptive")
2. **Top 3 emotions** with trend arrows (â†‘â†“â†’)
3. **Speech metrics** (WPM, filler words, pauses)
4. **Coaching advice** (1-2 sentences from GPT-4)

---

## Testing the Pipeline

### Quick Test (30 seconds)

1. Run `./start_demo.sh`
2. Open `http://localhost:8080`
3. Click **"Start Session"**
4. Allow camera and microphone access
5. Look at the camera and say something like:
   > "I think this is a great opportunity for growth"
6. Watch the UI update:
   - Emotion should show "Receptive" or "Positive"
   - Speech metrics should update
   - After 5 seconds, coaching advice appears

### Expected Behavior

**Immediate (0-1 seconds):**
- Webcam video appears
- Status shows "Active - Analyzing..."

**Within 1 second:**
- Emotion label updates (based on your facial expression)
- Confidence bar shows detection quality

**Within 5 seconds:**
- Speech metrics update (if you're speaking)
- Top emotions appear with trend arrows

**At 5 seconds:**
- First coaching advice appears
- Should say something relevant to your facial expression/speech

**Every 4.5 seconds after:**
- New coaching advice updates

---

## Troubleshooting

### "Failed to initialize audio"
- Grant microphone permission in browser
- Check browser console for errors

### "Connection failed" or "WebSocket error"
- Ensure backend is running: `curl http://localhost:8000/`
- Check `.env` file has `OPENAI_API_KEY` and `HUME_API_KEY`

### "No face detected"
- Ensure good lighting
- Face should be centered in camera
- Try moving closer to camera

### No coaching advice after 5 seconds
- Check backend logs for errors
- Verify OpenAI API key is valid
- Ensure at least one interval completed (check console logs)

---

## File Organization

```
BeneAI/
â”œâ”€â”€ README.md                    # Project overview
â”œâ”€â”€ USAGE_GUIDE.md              # Comprehensive usage guide (setup, testing, API)
â”œâ”€â”€ DATA_FLOW.md                # Technical data flow documentation
â”œâ”€â”€ EMOTION_TIMESERIES.md       # Time-series emotion system docs
â”œâ”€â”€ QUICK_START.md              # This file (quick start)
â”œâ”€â”€ start_demo.sh               # Demo startup script (WORKING)
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html              # Main UI (press "Start Session" here)
â”‚   â”œâ”€â”€ overlay.html            # Standalone overlay widget
â”‚   â”œâ”€â”€ js/
â”‚   â”‚   â”œâ”€â”€ app.js              # Main application logic
â”‚   â”‚   â”œâ”€â”€ video-processor.js  # Video frame capture
â”‚   â”‚   â”œâ”€â”€ audio-analyzer.js   # Speech analysis
â”‚   â”‚   â”œâ”€â”€ websocket-client.js # Backend communication
â”‚   â”‚   â””â”€â”€ config.js           # Configuration
â”‚   â””â”€â”€ css/
â”‚       â””â”€â”€ styles.css          # UI styles
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                 # FastAPI + WebSocket server
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ llm.py              # OpenAI GPT-4 integration
â”‚   â”‚   â”œâ”€â”€ hume_client.py      # Hume AI emotion detection
â”‚   â”‚   â”œâ”€â”€ interval_aggregator.py   # 1-second intervals
â”‚   â”‚   â”œâ”€â”€ timeseries_buffer.py     # 5-second buffer
â”‚   â”‚   â”œâ”€â”€ speech_mapper.py         # Speech-to-text alignment
â”‚   â”‚   â”œâ”€â”€ llm_context_builder.py   # LLM context formatting
â”‚   â”‚   â””â”€â”€ prompts.py          # GPT-4 prompts
â”‚   â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚   â””â”€â”€ .env                    # API keys (OPENAI_API_KEY, HUME_API_KEY)
```

---

## Next Steps

1. **Test the system:** Run `./start_demo.sh` and press "Start Session"
2. **Read technical docs:** Check `DATA_FLOW.md` for complete pipeline
3. **Understand the system:** Read `EMOTION_TIMESERIES.md` for design decisions
4. **Deploy (optional):** See `USAGE_GUIDE.md` for Cloud Run deployment

---

**Questions?** Check the other MD files or open an issue on GitHub.

**Pro tip:** Open DevTools Console (F12) to see real-time logs of data flowing through the system!
