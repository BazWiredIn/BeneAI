{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeneAI Performance Testing Notebook\n",
    "\n",
    "This notebook provides a comprehensive framework for testing BeneAI's end-to-end pipeline performance, including:\n",
    "- **Latency**: Time from frame capture to emotion detection\n",
    "- **Throughput**: Frames processed per second\n",
    "- **Accuracy**: Precision, recall, F1 score compared to ground truth annotations\n",
    "\n",
    "## Workflow\n",
    "1. Configure test parameters (frame rates, resolutions, MediaPipe settings)\n",
    "2. Load ground truth annotations\n",
    "3. Run parameter sweep tests\n",
    "4. Calculate metrics and visualize results\n",
    "5. Export results and recommendations\n",
    "\n",
    "## Prerequisites\n",
    "- BeneAI backend running at configured URL (default: ws://localhost:8000/ws)\n",
    "- Test video file in `videos/` directory\n",
    "- Ground truth annotations JSON file (use `ground_truth_template.json` as reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import nest_asyncio\n",
    "\n",
    "# Enable nested event loops for Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Import custom utilities\n",
    "from test_utils import (\n",
    "    TestConfig, TestResult, VideoProcessor, BeneAIClient,\n",
    "    GroundTruth, MetricsCalculator, run_single_test, aggregate_results\n",
    ")\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úì All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Paths\n",
    "    'video_path': 'videos/test_video.mp4',  # Update with your video filename\n",
    "    'ground_truth_path': 'videos/test_video_annotations.json',  # Update with your annotations\n",
    "    'results_dir': 'results/',\n",
    "    \n",
    "    # Backend\n",
    "    'backend_url': 'ws://localhost:8000/ws',\n",
    "    \n",
    "    # Test timestamp\n",
    "    'test_run_id': datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "}\n",
    "\n",
    "# Ensure paths exist\n",
    "Path(CONFIG['results_dir']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for testing\n",
    "PARAM_GRID = {\n",
    "    'frame_rates': [1, 2, 3, 5, 10],  # FPS to test\n",
    "    'resolutions': [\n",
    "        (320, 240),   # Low\n",
    "        (480, 360),   # Medium (current default)\n",
    "        (640, 480)    # High\n",
    "    ],\n",
    "    'jpeg_qualities': [60],  # Keep constant (0.6 * 100)\n",
    "    'mediapipe_complexities': [0, 1, 2],  # 0=fast, 1=medium, 2=accurate\n",
    "    'mediapipe_confidences': [0.3, 0.5, 0.7]  # Detection confidence thresholds\n",
    "}\n",
    "\n",
    "# Generate all test configurations\n",
    "def generate_test_configs(param_grid):\n",
    "    \"\"\"Generate all combinations of test parameters\"\"\"\n",
    "    configs = []\n",
    "    \n",
    "    for fps in param_grid['frame_rates']:\n",
    "        for res in param_grid['resolutions']:\n",
    "            for quality in param_grid['jpeg_qualities']:\n",
    "                for complexity in param_grid['mediapipe_complexities']:\n",
    "                    for confidence in param_grid['mediapipe_confidences']:\n",
    "                        config = TestConfig(\n",
    "                            frame_rate=fps,\n",
    "                            resolution=res,\n",
    "                            jpeg_quality=quality,\n",
    "                            mediapipe_complexity=complexity,\n",
    "                            mediapipe_confidence=confidence\n",
    "                        )\n",
    "                        configs.append(config)\n",
    "    \n",
    "    return configs\n",
    "\n",
    "TEST_CONFIGS = generate_test_configs(PARAM_GRID)\n",
    "\n",
    "print(f\"Generated {len(TEST_CONFIGS)} test configurations\")\n",
    "print(f\"\\nExample config: {TEST_CONFIGS[0].config_id()}\")\n",
    "print(f\"\\nEstimated time (assuming 30s video, ~500ms per frame):\")\n",
    "print(f\"  ~{len(TEST_CONFIGS) * 2:.0f}-{len(TEST_CONFIGS) * 5:.0f} minutes total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Ground Truth Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth annotations\n",
    "try:\n",
    "    ground_truth = GroundTruth(CONFIG['ground_truth_path'])\n",
    "    print(f\"‚úì Loaded {len(ground_truth.annotations)} annotations\")\n",
    "    print(f\"  Video: {ground_truth.video_name}\")\n",
    "    print(f\"  FPS: {ground_truth.fps}\")\n",
    "    print(f\"  Unique emotions: {len(ground_truth.unique_emotions)}\")\n",
    "    print(f\"  Unique states: {len(ground_truth.unique_states)}\")\n",
    "    \n",
    "    # Display annotations DataFrame\n",
    "    annotations_df = ground_truth.to_dataframe()\n",
    "    print(f\"\\nAnnotations preview:\")\n",
    "    display(annotations_df.head(10))\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ö† Ground truth file not found: {CONFIG['ground_truth_path']}\")\n",
    "    print(\"  Tests will run without accuracy metrics\")\n",
    "    print(\"  Use ground_truth_template.json to create annotations\")\n",
    "    ground_truth = None\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading ground truth: {e}\")\n",
    "    ground_truth = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ground truth timeline\n",
    "if ground_truth:\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "    \n",
    "    # Emotion timeline\n",
    "    ax1 = axes[0]\n",
    "    emotion_colors = {emotion: f\"C{i}\" for i, emotion in enumerate(ground_truth.unique_emotions)}\n",
    "    \n",
    "    for annotation in ground_truth.annotations:\n",
    "        ax1.axvline(annotation['timestamp'], color=emotion_colors[annotation['emotion']], \n",
    "                    alpha=0.6, linewidth=2, label=annotation['emotion'])\n",
    "    \n",
    "    # Remove duplicate labels\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    ax1.legend(by_label.values(), by_label.keys(), loc='upper right')\n",
    "    \n",
    "    ax1.set_ylabel('Emotions')\n",
    "    ax1.set_title('Ground Truth Emotion Timeline')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Investor state timeline\n",
    "    ax2 = axes[1]\n",
    "    state_colors = {state: f\"C{i}\" for i, state in enumerate(ground_truth.unique_states)}\n",
    "    \n",
    "    for annotation in ground_truth.annotations:\n",
    "        ax2.axvline(annotation['timestamp'], color=state_colors[annotation['investor_state']], \n",
    "                    alpha=0.6, linewidth=2, label=annotation['investor_state'])\n",
    "    \n",
    "    handles, labels = ax2.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    ax2.legend(by_label.values(), by_label.keys(), loc='upper right')\n",
    "    \n",
    "    ax2.set_xlabel('Time (seconds)')\n",
    "    ax2.set_ylabel('Investor State')\n",
    "    ax2.set_title('Ground Truth Investor State Timeline')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Distribution of emotions and states\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Emotion distribution\n",
    "    emotion_counts = annotations_df['emotion'].value_counts()\n",
    "    axes[0].bar(emotion_counts.index, emotion_counts.values, color='steelblue')\n",
    "    axes[0].set_xlabel('Emotion')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Emotion Distribution')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # State distribution\n",
    "    state_counts = annotations_df['investor_state'].value_counts()\n",
    "    axes[1].bar(state_counts.index, state_counts.values, color='coral')\n",
    "    axes[1].set_xlabel('Investor State')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].set_title('Investor State Distribution')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Backend Communication Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test backend connectivity\n",
    "async def test_backend_connection():\n",
    "    \"\"\"Verify backend is accessible\"\"\"\n",
    "    try:\n",
    "        async with BeneAIClient(CONFIG['backend_url']) as client:\n",
    "            print(f\"‚úì Successfully connected to {CONFIG['backend_url']}\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to connect to backend: {e}\")\n",
    "        print(f\"\\nMake sure the backend is running:\")\n",
    "        print(f\"  cd backend && python -m uvicorn app.main:app --reload\")\n",
    "        return False\n",
    "\n",
    "# Run connectivity test\n",
    "backend_available = await test_backend_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test with first frame\n",
    "if backend_available:\n",
    "    print(\"Running quick test with first frame...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Extract first frame\n",
    "        with VideoProcessor(CONFIG['video_path']) as vp:\n",
    "            print(f\"Video info:\")\n",
    "            print(f\"  FPS: {vp.original_fps:.2f}\")\n",
    "            print(f\"  Total frames: {vp.total_frames}\")\n",
    "            print(f\"  Duration: {vp.duration:.2f}s\\n\")\n",
    "            \n",
    "            frames = vp.extract_frames(target_fps=1, resolution=(480, 360))\n",
    "            if frames:\n",
    "                timestamp, frame = frames[0]\n",
    "                frame_b64 = vp.encode_frame_jpeg(frame, quality=60)\n",
    "                \n",
    "                # Send to backend\n",
    "                async with BeneAIClient(CONFIG['backend_url']) as client:\n",
    "                    response, latency_ms = await client.send_frame(frame_b64, timestamp)\n",
    "                    \n",
    "                print(f\"‚úì Test frame processed successfully\")\n",
    "                print(f\"  Latency: {latency_ms:.1f}ms\")\n",
    "                print(f\"  Response type: {response.get('type')}\")\n",
    "                \n",
    "                if response.get('type') == 'emotion_update':\n",
    "                    data = response.get('data', {})\n",
    "                    print(f\"  Emotion: {data.get('primary_emotion')}\")\n",
    "                    print(f\"  State: {data.get('investor_state')}\")\n",
    "                    print(f\"  Confidence: {data.get('confidence', 0):.2f}\")\n",
    "            else:\n",
    "                print(\"‚ùå No frames extracted from video\")\n",
    "                \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Video file not found: {CONFIG['video_path']}\")\n",
    "        print(f\"   Please add your test video to the videos/ directory\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during test: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Test Execution - Parameter Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Run all tests (can take a while)\n",
    "RUN_ALL_TESTS = False  # Set to True to run all configurations\n",
    "\n",
    "# Option 2: Run subset of tests for quick iteration\n",
    "RUN_SUBSET = True\n",
    "SUBSET_CONFIGS = TEST_CONFIGS[::10]  # Every 10th config\n",
    "\n",
    "# Option 3: Run specific configs\n",
    "RUN_SPECIFIC = False\n",
    "SPECIFIC_INDICES = [0, 10, 20, 30, 40]  # Specific config indices\n",
    "\n",
    "# Select configs to run\n",
    "if RUN_ALL_TESTS:\n",
    "    configs_to_run = TEST_CONFIGS\n",
    "    print(f\"Running ALL {len(configs_to_run)} test configurations\")\n",
    "elif RUN_SUBSET:\n",
    "    configs_to_run = SUBSET_CONFIGS\n",
    "    print(f\"Running SUBSET of {len(configs_to_run)} test configurations\")\n",
    "elif RUN_SPECIFIC:\n",
    "    configs_to_run = [TEST_CONFIGS[i] for i in SPECIFIC_INDICES if i < len(TEST_CONFIGS)]\n",
    "    print(f\"Running SPECIFIC {len(configs_to_run)} test configurations\")\n",
    "else:\n",
    "    configs_to_run = TEST_CONFIGS[:3]  # Just first 3 by default\n",
    "    print(f\"Running first {len(configs_to_run)} test configurations (default)\")\n",
    "\n",
    "print(f\"\\nConfigurations to test:\")\n",
    "for i, config in enumerate(configs_to_run[:5]):\n",
    "    print(f\"  {i+1}. {config.config_id()}\")\n",
    "if len(configs_to_run) > 5:\n",
    "    print(f\"  ... and {len(configs_to_run) - 5} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tests with progress tracking\n",
    "async def run_all_tests(configs, video_path, backend_url, ground_truth):\n",
    "    \"\"\"Run all test configurations\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, config in enumerate(tqdm(configs, desc=\"Running tests\")):\n",
    "        print(f\"\\n[{i+1}/{len(configs)}] Testing: {config.config_id()}\")\n",
    "        \n",
    "        try:\n",
    "            result = await run_single_test(\n",
    "                video_path=video_path,\n",
    "                config=config,\n",
    "                backend_url=backend_url,\n",
    "                ground_truth=ground_truth,\n",
    "                progress_callback=lambda current, total: None  # Silent progress\n",
    "            )\n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Test failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run tests if backend is available\n",
    "if backend_available:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STARTING TEST EXECUTION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    test_results = await run_all_tests(\n",
    "        configs=configs_to_run,\n",
    "        video_path=CONFIG['video_path'],\n",
    "        backend_url=CONFIG['backend_url'],\n",
    "        ground_truth=ground_truth\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"‚úì COMPLETED {len(test_results)}/{len(configs_to_run)} TESTS\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"‚ö† Backend not available. Skipping tests.\")\n",
    "    test_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Metrics Calculation & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results into DataFrame\n",
    "if test_results:\n",
    "    results_df = aggregate_results(test_results)\n",
    "    \n",
    "    print(f\"Results Summary ({len(results_df)} tests):\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(\"\\nLatency Statistics (ms):\")\n",
    "    print(results_df[['avg_latency_ms', 'p95_latency_ms']].describe())\n",
    "    \n",
    "    print(\"\\nThroughput Statistics (fps):\")\n",
    "    print(results_df['throughput_fps'].describe())\n",
    "    \n",
    "    if 'f1_score' in results_df.columns and results_df['f1_score'].notna().any():\n",
    "        print(\"\\nAccuracy Statistics (F1 Score):\")\n",
    "        print(results_df['f1_score'].describe())\n",
    "    \n",
    "    # Display top performing configurations\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Top 5 Configurations by F1 Score:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if 'f1_score' in results_df.columns:\n",
    "        top_by_f1 = results_df.nlargest(5, 'f1_score')[[\n",
    "            'config_id', 'frame_rate', 'resolution', 'mediapipe_complexity',\n",
    "            'f1_score', 'avg_latency_ms', 'throughput_fps'\n",
    "        ]]\n",
    "        display(top_by_f1)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Top 5 Configurations by Latency (lowest):\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    top_by_latency = results_df.nsmallest(5, 'avg_latency_ms')[[\n",
    "        'config_id', 'frame_rate', 'resolution', 'mediapipe_complexity',\n",
    "        'avg_latency_ms', 'p95_latency_ms', 'throughput_fps'\n",
    "    ]]\n",
    "    display(top_by_latency)\n",
    "    \n",
    "    # Full results table\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"All Results:\")\n",
    "    print(\"=\"*60)\n",
    "    display(results_df)\n",
    "    \n",
    "else:\n",
    "    print(\"No test results available\")\n",
    "    results_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Visualization & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latency Distribution\n",
    "if test_results and results_df is not None:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Latency by Frame Rate\n",
    "    ax1 = axes[0, 0]\n",
    "    results_df.boxplot(column='avg_latency_ms', by='frame_rate', ax=ax1)\n",
    "    ax1.set_xlabel('Frame Rate (FPS)')\n",
    "    ax1.set_ylabel('Average Latency (ms)')\n",
    "    ax1.set_title('Latency by Frame Rate')\n",
    "    plt.sca(ax1)\n",
    "    plt.xticks(rotation=0)\n",
    "    \n",
    "    # 2. Latency by Resolution\n",
    "    ax2 = axes[0, 1]\n",
    "    results_df.boxplot(column='avg_latency_ms', by='resolution', ax=ax2)\n",
    "    ax2.set_xlabel('Resolution')\n",
    "    ax2.set_ylabel('Average Latency (ms)')\n",
    "    ax2.set_title('Latency by Resolution')\n",
    "    plt.sca(ax2)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # 3. Latency by MediaPipe Complexity\n",
    "    ax3 = axes[1, 0]\n",
    "    results_df.boxplot(column='avg_latency_ms', by='mediapipe_complexity', ax=ax3)\n",
    "    ax3.set_xlabel('MediaPipe Complexity')\n",
    "    ax3.set_ylabel('Average Latency (ms)')\n",
    "    ax3.set_title('Latency by MediaPipe Complexity')\n",
    "    plt.sca(ax3)\n",
    "    plt.xticks(rotation=0)\n",
    "    \n",
    "    # 4. Latency by MediaPipe Confidence\n",
    "    ax4 = axes[1, 1]\n",
    "    results_df.boxplot(column='avg_latency_ms', by='mediapipe_confidence', ax=ax4)\n",
    "    ax4.set_xlabel('MediaPipe Confidence Threshold')\n",
    "    ax4.set_ylabel('Average Latency (ms)')\n",
    "    ax4.set_title('Latency by Confidence Threshold')\n",
    "    plt.sca(ax4)\n",
    "    plt.xticks(rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{CONFIG['results_dir']}/latency_analysis_{CONFIG['test_run_id']}.png\", dpi=150)\n",
    "    plt.show()\n",
    "    print(\"‚úì Saved: latency_analysis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throughput Analysis\n",
    "if test_results and results_df is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Throughput by frame rate\n",
    "    ax1 = axes[0]\n",
    "    for resolution in results_df['resolution'].unique():\n",
    "        subset = results_df[results_df['resolution'] == resolution]\n",
    "        ax1.plot(subset['frame_rate'], subset['throughput_fps'], \n",
    "                marker='o', label=resolution, alpha=0.7)\n",
    "    ax1.set_xlabel('Target Frame Rate (FPS)')\n",
    "    ax1.set_ylabel('Actual Throughput (FPS)')\n",
    "    ax1.set_title('Throughput vs Target Frame Rate')\n",
    "    ax1.legend(title='Resolution')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Throughput distribution\n",
    "    ax2 = axes[1]\n",
    "    ax2.hist(results_df['throughput_fps'], bins=20, color='steelblue', edgecolor='black')\n",
    "    ax2.axvline(results_df['throughput_fps'].mean(), color='red', \n",
    "               linestyle='--', label=f\"Mean: {results_df['throughput_fps'].mean():.2f}\")\n",
    "    ax2.set_xlabel('Throughput (FPS)')\n",
    "    ax2.set_ylabel('Count')\n",
    "    ax2.set_title('Throughput Distribution')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{CONFIG['results_dir']}/throughput_analysis_{CONFIG['test_run_id']}.png\", dpi=150)\n",
    "    plt.show()\n",
    "    print(\"‚úì Saved: throughput_analysis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Analysis (if ground truth available)\n",
    "if test_results and results_df is not None and 'f1_score' in results_df.columns:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. F1 Score by Frame Rate\n",
    "    ax1 = axes[0, 0]\n",
    "    results_df.boxplot(column='f1_score', by='frame_rate', ax=ax1)\n",
    "    ax1.set_xlabel('Frame Rate (FPS)')\n",
    "    ax1.set_ylabel('F1 Score')\n",
    "    ax1.set_title('F1 Score by Frame Rate')\n",
    "    plt.sca(ax1)\n",
    "    plt.xticks(rotation=0)\n",
    "    \n",
    "    # 2. F1 Score by Resolution\n",
    "    ax2 = axes[0, 1]\n",
    "    results_df.boxplot(column='f1_score', by='resolution', ax=ax2)\n",
    "    ax2.set_xlabel('Resolution')\n",
    "    ax2.set_ylabel('F1 Score')\n",
    "    ax2.set_title('F1 Score by Resolution')\n",
    "    plt.sca(ax2)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # 3. F1 Score by MediaPipe Complexity\n",
    "    ax3 = axes[1, 0]\n",
    "    results_df.boxplot(column='f1_score', by='mediapipe_complexity', ax=ax3)\n",
    "    ax3.set_xlabel('MediaPipe Complexity')\n",
    "    ax3.set_ylabel('F1 Score')\n",
    "    ax3.set_title('F1 Score by MediaPipe Complexity')\n",
    "    plt.sca(ax3)\n",
    "    plt.xticks(rotation=0)\n",
    "    \n",
    "    # 4. Precision vs Recall scatter\n",
    "    ax4 = axes[1, 1]\n",
    "    scatter = ax4.scatter(results_df['recall'], results_df['precision'], \n",
    "                         c=results_df['f1_score'], cmap='viridis', s=100, alpha=0.6)\n",
    "    ax4.set_xlabel('Recall')\n",
    "    ax4.set_ylabel('Precision')\n",
    "    ax4.set_title('Precision vs Recall')\n",
    "    ax4.plot([0, 1], [0, 1], 'r--', alpha=0.3, label='Perfect balance')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=ax4, label='F1 Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{CONFIG['results_dir']}/accuracy_analysis_{CONFIG['test_run_id']}.png\", dpi=150)\n",
    "    plt.show()\n",
    "    print(\"‚úì Saved: accuracy_analysis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latency vs Accuracy Tradeoff\n",
    "if test_results and results_df is not None and 'f1_score' in results_df.columns:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Color by frame rate, size by resolution\n",
    "    for fps in results_df['frame_rate'].unique():\n",
    "        subset = results_df[results_df['frame_rate'] == fps]\n",
    "        ax.scatter(subset['avg_latency_ms'], subset['f1_score'], \n",
    "                  label=f\"{fps} FPS\", s=100, alpha=0.6)\n",
    "    \n",
    "    ax.set_xlabel('Average Latency (ms)')\n",
    "    ax.set_ylabel('F1 Score (Accuracy)')\n",
    "    ax.set_title('Latency vs Accuracy Tradeoff')\n",
    "    ax.legend(title='Frame Rate')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Highlight best tradeoff (Pareto optimal)\n",
    "    best_idx = (results_df['f1_score'] / results_df['f1_score'].max() - \n",
    "                results_df['avg_latency_ms'] / results_df['avg_latency_ms'].max()).idxmax()\n",
    "    best_config = results_df.loc[best_idx]\n",
    "    ax.scatter(best_config['avg_latency_ms'], best_config['f1_score'], \n",
    "              color='red', s=300, marker='*', edgecolors='black', linewidths=2,\n",
    "              label='Best Tradeoff', zorder=10)\n",
    "    \n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{CONFIG['results_dir']}/latency_accuracy_tradeoff_{CONFIG['test_run_id']}.png\", dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Best Tradeoff Configuration:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Config ID: {best_config['config_id']}\")\n",
    "    print(f\"Frame Rate: {best_config['frame_rate']} FPS\")\n",
    "    print(f\"Resolution: {best_config['resolution']}\")\n",
    "    print(f\"MediaPipe Complexity: {best_config['mediapipe_complexity']}\")\n",
    "    print(f\"MediaPipe Confidence: {best_config['mediapipe_confidence']}\")\n",
    "    print(f\"\\nMetrics:\")\n",
    "    print(f\"  Latency: {best_config['avg_latency_ms']:.1f}ms (p95: {best_config['p95_latency_ms']:.1f}ms)\")\n",
    "    print(f\"  Throughput: {best_config['throughput_fps']:.2f} fps\")\n",
    "    print(f\"  F1 Score: {best_config['f1_score']:.3f}\")\n",
    "    print(f\"  Precision: {best_config['precision']:.3f}\")\n",
    "    print(f\"  Recall: {best_config['recall']:.3f}\")\n",
    "    print(\"‚úì Saved: latency_accuracy_tradeoff.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for Best Configuration\n",
    "if test_results:\n",
    "    # Find best result by F1 score\n",
    "    if results_df is not None and 'f1_score' in results_df.columns:\n",
    "        best_idx = results_df['f1_score'].idxmax()\n",
    "        best_result = test_results[best_idx]\n",
    "        \n",
    "        if best_result.confusion_mat is not None:\n",
    "            fig, ax = plt.subplots(figsize=(10, 8))\n",
    "            \n",
    "            # Get labels (emotions)\n",
    "            y_true, y_pred = MetricsCalculator.align_predictions_with_ground_truth(\n",
    "                best_result.emotion_results, ground_truth, tolerance=0.5\n",
    "            )\n",
    "            labels = sorted(set(y_true))\n",
    "            \n",
    "            # Plot confusion matrix\n",
    "            sns.heatmap(best_result.confusion_mat, annot=True, fmt='d', cmap='Blues',\n",
    "                       xticklabels=labels, yticklabels=labels, ax=ax)\n",
    "            ax.set_xlabel('Predicted Emotion')\n",
    "            ax.set_ylabel('True Emotion')\n",
    "            ax.set_title(f'Confusion Matrix - Best Configuration\\n{best_result.config.config_id()}')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{CONFIG['results_dir']}/confusion_matrix_{CONFIG['test_run_id']}.png\", dpi=150)\n",
    "            plt.show()\n",
    "            print(\"‚úì Saved: confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Results Export & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to CSV\n",
    "if results_df is not None:\n",
    "    csv_path = f\"{CONFIG['results_dir']}/test_results_{CONFIG['test_run_id']}.csv\"\n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "    print(f\"‚úì Exported results to: {csv_path}\")\n",
    "    \n",
    "    # Export detailed results as JSON\n",
    "    json_path = f\"{CONFIG['results_dir']}/test_results_detailed_{CONFIG['test_run_id']}.json\"\n",
    "    \n",
    "    detailed_results = []\n",
    "    for result in test_results:\n",
    "        result_dict = result.to_dict()\n",
    "        # Convert emotion results to dicts\n",
    "        result_dict['emotion_results'] = [\n",
    "            {\n",
    "                'timestamp': er.timestamp,\n",
    "                'emotion': er.emotion,\n",
    "                'investor_state': er.investor_state,\n",
    "                'confidence': er.confidence,\n",
    "                'latency_ms': er.latency_ms\n",
    "            }\n",
    "            for er in result.emotion_results\n",
    "        ]\n",
    "        detailed_results.append(result_dict)\n",
    "    \n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'test_run_id': CONFIG['test_run_id'],\n",
    "            'video_path': CONFIG['video_path'],\n",
    "            'ground_truth_path': CONFIG['ground_truth_path'],\n",
    "            'total_tests': len(test_results),\n",
    "            'results': detailed_results\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úì Exported detailed results to: {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations summary\n",
    "if results_df is not None:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RECOMMENDATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Best for latency\n",
    "    best_latency = results_df.loc[results_df['avg_latency_ms'].idxmin()]\n",
    "    print(\"\\nüöÄ LOWEST LATENCY Configuration:\")\n",
    "    print(f\"   {best_latency['config_id']}\")\n",
    "    print(f\"   Latency: {best_latency['avg_latency_ms']:.1f}ms avg\")\n",
    "    print(f\"   Use when: Responsiveness is critical\")\n",
    "    \n",
    "    # Best for accuracy (if available)\n",
    "    if 'f1_score' in results_df.columns and results_df['f1_score'].notna().any():\n",
    "        best_accuracy = results_df.loc[results_df['f1_score'].idxmax()]\n",
    "        print(\"\\nüéØ HIGHEST ACCURACY Configuration:\")\n",
    "        print(f\"   {best_accuracy['config_id']}\")\n",
    "        print(f\"   F1 Score: {best_accuracy['f1_score']:.3f}\")\n",
    "        print(f\"   Use when: Accuracy is most important\")\n",
    "        \n",
    "        # Best balanced (if F1 scores available)\n",
    "        # Normalize and combine metrics\n",
    "        normalized_f1 = results_df['f1_score'] / results_df['f1_score'].max()\n",
    "        normalized_latency = 1 - (results_df['avg_latency_ms'] / results_df['avg_latency_ms'].max())\n",
    "        balance_score = (normalized_f1 + normalized_latency) / 2\n",
    "        best_balanced = results_df.loc[balance_score.idxmax()]\n",
    "        \n",
    "        print(\"\\n‚öñÔ∏è  BEST BALANCED Configuration:\")\n",
    "        print(f\"   {best_balanced['config_id']}\")\n",
    "        print(f\"   F1 Score: {best_balanced['f1_score']:.3f}\")\n",
    "        print(f\"   Latency: {best_balanced['avg_latency_ms']:.1f}ms avg\")\n",
    "        print(f\"   Use when: Need good balance of speed and accuracy\")\n",
    "    \n",
    "    # Best for throughput\n",
    "    best_throughput = results_df.loc[results_df['throughput_fps'].idxmax()]\n",
    "    print(\"\\nüìä HIGHEST THROUGHPUT Configuration:\")\n",
    "    print(f\"   {best_throughput['config_id']}\")\n",
    "    print(f\"   Throughput: {best_throughput['throughput_fps']:.2f} fps\")\n",
    "    print(f\"   Use when: Processing many frames quickly\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Save recommendations\n",
    "    recommendations_path = f\"{CONFIG['results_dir']}/recommendations_{CONFIG['test_run_id']}.txt\"\n",
    "    with open(recommendations_path, 'w') as f:\n",
    "        f.write(\"BeneAI Performance Testing Recommendations\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\\n\")\n",
    "        f.write(f\"Test Run ID: {CONFIG['test_run_id']}\\n\")\n",
    "        f.write(f\"Video: {CONFIG['video_path']}\\n\")\n",
    "        f.write(f\"Total Tests: {len(test_results)}\\n\\n\")\n",
    "        f.write(f\"Lowest Latency: {best_latency['config_id']}\\n\")\n",
    "        if 'f1_score' in results_df.columns:\n",
    "            f.write(f\"Highest Accuracy: {best_accuracy['config_id']}\\n\")\n",
    "            f.write(f\"Best Balanced: {best_balanced['config_id']}\\n\")\n",
    "        f.write(f\"Highest Throughput: {best_throughput['config_id']}\\n\")\n",
    "    \n",
    "    print(f\"\\n‚úì Saved recommendations to: {recommendations_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook provided comprehensive testing of BeneAI's performance across multiple parameter configurations. \n",
    "\n",
    "**Key Outputs:**\n",
    "- Performance metrics (latency, throughput, accuracy)\n",
    "- Visualizations showing parameter impacts\n",
    "- Configuration recommendations for different use cases\n",
    "- Exported results for further analysis\n",
    "\n",
    "**Next Steps:**\n",
    "1. Apply recommended configurations to your production deployment\n",
    "2. Re-run tests with additional videos to validate findings\n",
    "3. Monitor real-world performance and adjust as needed\n",
    "4. Consider A/B testing different configurations with users"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
